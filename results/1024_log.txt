+ export VLLM_ATTENTION_BACKEND=XFORMERS
+ VLLM_ATTENTION_BACKEND=XFORMERS
+ MODEL_PATH=l3lab/L1-Qwen-1.5B-Exact
+ NUM_TOKENS=512
+ MAX_TOKENS=1024
+ DATATYPES=("gpqa" "mmlu_1000" "lsat" "aime2025" "math" "amc" "aime" "olympiad_bench")
++ nvidia-smi --query-gpu=name --format=csv,noheader
++ wc -l
+ n_gpus_per_node=8
+ echo 'n_gpus_per_node: 8'
n_gpus_per_node: 8
+ : /lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output
+ : /lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/data
+ [[ 7 -gt 0 ]]
+ case $1 in
+ NUM_TOKENS=1024
+ MAX_TOKENS=2048
+ shift 2
+ [[ 5 -gt 0 ]]
+ case $1 in
+ shift
+ DATATYPES=()
+ [[ 4 -gt 0 ]]
+ [[ ! aime2025 =~ ^-- ]]
+ DATATYPES+=("$1")
+ shift
+ [[ 3 -gt 0 ]]
+ [[ ! gpqa =~ ^-- ]]
+ DATATYPES+=("$1")
+ shift
+ [[ 2 -gt 0 ]]
+ [[ ! lsat =~ ^-- ]]
+ DATATYPES+=("$1")
+ shift
+ [[ 1 -gt 0 ]]
+ [[ ! mmlu_1000 =~ ^-- ]]
+ DATATYPES+=("$1")
+ shift
+ [[ 0 -gt 0 ]]
+ [[ 0 -gt 0 ]]
+ echo 'Model Path: l3lab/L1-Qwen-1.5B-Exact'
Model Path: l3lab/L1-Qwen-1.5B-Exact
+ echo 'Datasets: aime2025' gpqa lsat mmlu_1000
Datasets: aime2025 gpqa lsat mmlu_1000
+ echo 'Output Directory: /lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output'
Output Directory: /lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output
+ echo 'Number of Tokens: 1024'
Number of Tokens: 1024
+ echo 'Max Tokens: 2048'
Max Tokens: 2048
+ for DATA_TYPE in "${DATATYPES[@]}"
++ date
+ echo 'Starting generation for aime2025 at Wed Apr 23 03:29:17 AM UTC 2025'
Starting generation for aime2025 at Wed Apr 23 03:29:17 AM UTC 2025
++ date +%s
+ start_time=1745378957
+ mkdir -p /lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024
+ python3 -m verl.trainer.main_generation trainer.nnodes=1 trainer.n_gpus_per_node=8 data.path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/data/data_1024/aime2025.parquet data.output_path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024/aime2025.parquet data.n_samples=16 data.batch_size=2048 model.path=l3lab/L1-Qwen-1.5B-Exact rollout.temperature=0.6 rollout.response_length=2048 rollout.top_k=-1 rollout.top_p=0.95 rollout.gpu_memory_utilization=0.9 rollout.tensor_model_parallel_size=1
/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
2025-04-23 03:29:25,972	INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.140.49.233:6379...
2025-04-23 03:29:25,985	INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[36m(pid=3645545)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3645545)[0m No module named 'vllm._version'
[36m(pid=3645545)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3645828)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3645828)[0m No module named 'vllm._version'
[36m(pid=3645828)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3645831)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3645831)[0m No module named 'vllm._version'
[36m(pid=3645831)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(ActorRolloutRefWorker pid=3645828)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3645828)[0m ERROR:2025-04-23 03:29:42,740:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3645828)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3645828)[0m ERROR:2025-04-23 03:29:42,821:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3645828)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(ActorRolloutRefWorker pid=3645828)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ActorRolloutRefWorker pid=3645828)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.58it/s]
[36m(ActorRolloutRefWorker pid=3645828)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.90it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.84it/s]
[36m(pid=3645830)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=3645830)[0m No module named 'vllm._version'[32m [repeated 5x across cluster][0m
[36m(pid=3645830)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 5x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645828)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 15x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645828)[0m ERROR:2025-04-23 03:30:03,055:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 15x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645826)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645826)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645545)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.16s/it][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645545)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.54s/it][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645832)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(ActorRolloutRefWorker pid=3645832)[0m   warnings.warn(
[36m(ActorRolloutRefWorker pid=3645545)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645545)[0m ERROR:2025-04-23 03:30:03,422:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 7x across cluster][0m
{'actor': {'fsdp_config': {'fsdp_size': -1,
                           'grad_offload': False,
                           'optimizer_offload': False,
                           'param_offload': False,
                           'wrap_policy': {'min_num_params': 0}},
           'optim': {'lr': 1e-06,
                     'lr_warmup_steps_ratio': 0.0,
                     'min_lr_ratio': None,
                     'total_training_steps': -1,
                     'warmup_style': 'constant'},
           'strategy': 'fsdp',
           'ulysses_sequence_parallel_size': 1},
 'data': {'batch_size': 2048,
          'data_source_key': 'data_source',
          'n_samples': 16,
          'output_path': '/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024/aime2025.parquet',
          'path': '/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/data/data_1024/aime2025.parquet',
          'prompt_key': 'prompt',
          'response_key': 'responses',
          'reward_model_key': 'reward_model'},
 'model': {'external_lib': None, 'path': 'l3lab/L1-Qwen-1.5B-Exact'},
 'rollout': {'do_sample': True,
             'dtype': 'bfloat16',
             'enable_chunked_prefill': True,
             'enforce_eager': True,
             'free_cache_engine': True,
             'gpu_memory_utilization': 0.9,
             'ignore_eos': False,
             'load_format': 'dummy_dtensor',
             'log_prob_micro_batch_size': 8,
             'max_num_batched_tokens': 8192,
             'max_num_seqs': 1024,
             'micro_batch_size': 256,
             'n': 1,
             'n_val': 1,
             'name': 'vllm',
             'prompt_length': 1536,
             'response_length': 2048,
             'temperature': 0.6,
             'tensor_model_parallel_size': 1,
             'top_k': -1,
             'top_p': 0.95},
 'trainer': {'n_gpus_per_node': 8, 'nnodes': 1}}
[36m(ActorRolloutRefWorker pid=3645545)[0m Model config after override: Qwen2Config {
[36m(ActorRolloutRefWorker pid=3645545)[0m   "_name_or_path": "l3lab/L1-Qwen-1.5B-Exact",
[36m(ActorRolloutRefWorker pid=3645545)[0m   "architectures": [
[36m(ActorRolloutRefWorker pid=3645545)[0m     "Qwen2ForCausalLM"
[36m(ActorRolloutRefWorker pid=3645545)[0m   ],
[36m(ActorRolloutRefWorker pid=3645545)[0m   "attention_dropout": 0.0,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "bos_token_id": 151646,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "eos_token_id": 151643,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "hidden_act": "silu",
[36m(ActorRolloutRefWorker pid=3645545)[0m   "hidden_size": 1536,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "initializer_range": 0.02,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "intermediate_size": 8960,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "max_position_embeddings": 131072,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "max_window_layers": 21,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "model_type": "qwen2",
[36m(ActorRolloutRefWorker pid=3645545)[0m   "num_attention_heads": 12,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "num_hidden_layers": 28,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "num_key_value_heads": 2,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "pad_token_id": 151643,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "rms_norm_eps": 1e-06,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "rope_scaling": null,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "rope_theta": 10000,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "sliding_window": null,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "tie_word_embeddings": false,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "torch_dtype": "float32",
[36m(ActorRolloutRefWorker pid=3645545)[0m   "transformers_version": "4.47.1",
[36m(ActorRolloutRefWorker pid=3645545)[0m   "use_cache": true,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "use_mrope": false,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "use_sliding_window": false,
[36m(ActorRolloutRefWorker pid=3645545)[0m   "vocab_size": 151936
[36m(ActorRolloutRefWorker pid=3645545)[0m }
[36m(ActorRolloutRefWorker pid=3645545)[0m 
[36m(ActorRolloutRefWorker pid=3645830)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x14952d6127a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(ActorRolloutRefWorker pid=3645545)[0m Qwen2ForCausalLM contains 1.78B parameters
[36m(ActorRolloutRefWorker pid=3645830)[0m Total steps: -1, num_warmup_steps: 0
[36m(ActorRolloutRefWorker pid=3645828)[0m INFO 04-23 03:30:03 config.py:1670] Downcasting torch.float32 to torch.bfloat16.
[36m(ActorRolloutRefWorker pid=3645545)[0m Before building vllm rollout, memory allocated (GB): 0.4375143051147461, memory reserved (GB): 3.2421875
[36m(ActorRolloutRefWorker pid=3645828)[0m INFO 04-23 03:30:09 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(ActorRolloutRefWorker pid=3645828)[0m WARNING 04-23 03:30:09 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(ActorRolloutRefWorker pid=3645826)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x14edb5cd67a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645545)[0m Total steps: -1, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645545)[0m INFO 04-23 03:30:03 config.py:1670] Downcasting torch.float32 to torch.bfloat16.[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645828)[0m local rank 0
[36m(ActorRolloutRefWorker pid=3645545)[0m before init cache memory allocated: 4.071012352GB, reserved: 4.223664128GB
[36m(ActorRolloutRefWorker pid=3645545)[0m after init cache memory allocated: 35.251468288GB, reserved: 35.404120064GB
[36m(ActorRolloutRefWorker pid=3645545)[0m INFO 04-23 03:30:11 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645545)[0m WARNING 04-23 03:30:11 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645545)[0m local rank 0[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645832)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}
[36m(ActorRolloutRefWorker pid=3645545)[0m After building vllm rollout, memory allocated (GB): 29.51576519012451, memory reserved (GB): 32.97265625
[36m(ActorRolloutRefWorker pid=3645545)[0m After building sharding manager, memory allocated (GB): 29.51576519012451, memory reserved (GB): 32.97265625
[1/1] Start to process.
[1/1] Start to generate.
480
[2025-04-23 03:31:09,338][datasets][INFO] - PyTorch version 2.4.0+cu124 available.
+------------+--------------------------+
| Metric     | Value                    |
+============+==========================+
| model_path | l3lab/L1-Qwen-1.5B-Exact |
+------------+--------------------------+
| dataset    | aime2025.parquet         |
+------------+--------------------------+
| pass@1     | 0.11875                  |
+------------+--------------------------+
| pass@16    | 0.23333333333333334      |
+------------+--------------------------+
[36m(ActorRolloutRefWorker pid=3645830)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645830)[0m   warnings.warn([32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3645829)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
++ date +%s
+ end_time=1745379072
+ duration=115
++ date
+ echo 'Finished generation for aime2025 at Wed Apr 23 03:31:12 AM UTC 2025'
Finished generation for aime2025 at Wed Apr 23 03:31:12 AM UTC 2025
+ echo 'Total generation time for aime2025: 115 seconds'
Total generation time for aime2025: 115 seconds
+ echo ----------------------------------------
----------------------------------------
+ for DATA_TYPE in "${DATATYPES[@]}"
++ date
+ echo 'Starting generation for gpqa at Wed Apr 23 03:31:12 AM UTC 2025'
Starting generation for gpqa at Wed Apr 23 03:31:12 AM UTC 2025
++ date +%s
+ start_time=1745379072
+ mkdir -p /lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024
+ python3 -m verl.trainer.main_generation trainer.nnodes=1 trainer.n_gpus_per_node=8 data.path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/data/data_1024/gpqa.parquet data.output_path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024/gpqa.parquet data.n_samples=16 data.batch_size=2048 model.path=l3lab/L1-Qwen-1.5B-Exact rollout.temperature=0.6 rollout.response_length=2048 rollout.top_k=-1 rollout.top_p=0.95 rollout.gpu_memory_utilization=0.9 rollout.tensor_model_parallel_size=1
/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
2025-04-23 03:31:29,715	INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.140.49.233:6379...
2025-04-23 03:31:29,730	INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[36m(pid=3656321)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3656321)[0m No module named 'vllm._version'
[36m(pid=3656321)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3656847)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3656847)[0m No module named 'vllm._version'
[36m(pid=3656847)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(ActorRolloutRefWorker pid=3656847)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3656847)[0m ERROR:2025-04-23 03:31:46,082:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3656847)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3656847)[0m ERROR:2025-04-23 03:31:46,161:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3656847)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(pid=3656848)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3656848)[0m No module named 'vllm._version'
[36m(pid=3656848)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(ActorRolloutRefWorker pid=3656847)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ActorRolloutRefWorker pid=3656847)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.93it/s]
[36m(ActorRolloutRefWorker pid=3656847)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.84it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.67it/s]
[36m(ActorRolloutRefWorker pid=3656850)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(ActorRolloutRefWorker pid=3656850)[0m ERROR:2025-04-23 03:32:06,825:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 15x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656321)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(pid=3656852)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 5x across cluster][0m
[36m(pid=3656852)[0m No module named 'vllm._version'[32m [repeated 5x across cluster][0m
[36m(pid=3656852)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 5x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656321)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656321)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.02s/it][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656321)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.44s/it][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656852)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(ActorRolloutRefWorker pid=3656852)[0m   warnings.warn(
[36m(ActorRolloutRefWorker pid=3656321)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656321)[0m ERROR:2025-04-23 03:32:07,346:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 7x across cluster][0m
{'actor': {'fsdp_config': {'fsdp_size': -1,
                           'grad_offload': False,
                           'optimizer_offload': False,
                           'param_offload': False,
                           'wrap_policy': {'min_num_params': 0}},
           'optim': {'lr': 1e-06,
                     'lr_warmup_steps_ratio': 0.0,
                     'min_lr_ratio': None,
                     'total_training_steps': -1,
                     'warmup_style': 'constant'},
           'strategy': 'fsdp',
           'ulysses_sequence_parallel_size': 1},
 'data': {'batch_size': 2048,
          'data_source_key': 'data_source',
          'n_samples': 16,
          'output_path': '/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024/gpqa.parquet',
          'path': '/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/data/data_1024/gpqa.parquet',
          'prompt_key': 'prompt',
          'response_key': 'responses',
          'reward_model_key': 'reward_model'},
 'model': {'external_lib': None, 'path': 'l3lab/L1-Qwen-1.5B-Exact'},
 'rollout': {'do_sample': True,
             'dtype': 'bfloat16',
             'enable_chunked_prefill': True,
             'enforce_eager': True,
             'free_cache_engine': True,
             'gpu_memory_utilization': 0.9,
             'ignore_eos': False,
             'load_format': 'dummy_dtensor',
             'log_prob_micro_batch_size': 8,
             'max_num_batched_tokens': 8192,
             'max_num_seqs': 1024,
             'micro_batch_size': 256,
             'n': 1,
             'n_val': 1,
             'name': 'vllm',
             'prompt_length': 1536,
             'response_length': 2048,
             'temperature': 0.6,
             'tensor_model_parallel_size': 1,
             'top_k': -1,
             'top_p': 0.95},
 'trainer': {'n_gpus_per_node': 8, 'nnodes': 1}}
[36m(ActorRolloutRefWorker pid=3656321)[0m Model config after override: Qwen2Config {
[36m(ActorRolloutRefWorker pid=3656321)[0m   "_name_or_path": "l3lab/L1-Qwen-1.5B-Exact",
[36m(ActorRolloutRefWorker pid=3656321)[0m   "architectures": [
[36m(ActorRolloutRefWorker pid=3656321)[0m     "Qwen2ForCausalLM"
[36m(ActorRolloutRefWorker pid=3656321)[0m   ],
[36m(ActorRolloutRefWorker pid=3656321)[0m   "attention_dropout": 0.0,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "bos_token_id": 151646,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "eos_token_id": 151643,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "hidden_act": "silu",
[36m(ActorRolloutRefWorker pid=3656321)[0m   "hidden_size": 1536,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "initializer_range": 0.02,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "intermediate_size": 8960,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "max_position_embeddings": 131072,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "max_window_layers": 21,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "model_type": "qwen2",
[36m(ActorRolloutRefWorker pid=3656321)[0m   "num_attention_heads": 12,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "num_hidden_layers": 28,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "num_key_value_heads": 2,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "pad_token_id": 151643,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "rms_norm_eps": 1e-06,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "rope_scaling": null,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "rope_theta": 10000,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "sliding_window": null,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "tie_word_embeddings": false,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "torch_dtype": "float32",
[36m(ActorRolloutRefWorker pid=3656321)[0m   "transformers_version": "4.47.1",
[36m(ActorRolloutRefWorker pid=3656321)[0m   "use_cache": true,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "use_mrope": false,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "use_sliding_window": false,
[36m(ActorRolloutRefWorker pid=3656321)[0m   "vocab_size": 151936
[36m(ActorRolloutRefWorker pid=3656321)[0m }
[36m(ActorRolloutRefWorker pid=3656321)[0m 
[36m(ActorRolloutRefWorker pid=3656321)[0m Qwen2ForCausalLM contains 1.78B parameters
[36m(ActorRolloutRefWorker pid=3656321)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x14845135a7a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(ActorRolloutRefWorker pid=3656850)[0m Total steps: -1, num_warmup_steps: 0
[36m(ActorRolloutRefWorker pid=3656850)[0m INFO 04-23 03:32:06 config.py:1670] Downcasting torch.float32 to torch.bfloat16.
[36m(ActorRolloutRefWorker pid=3656321)[0m Before building vllm rollout, memory allocated (GB): 0.4375143051147461, memory reserved (GB): 3.2421875
[36m(ActorRolloutRefWorker pid=3656851)[0m INFO 04-23 03:32:13 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(ActorRolloutRefWorker pid=3656851)[0m WARNING 04-23 03:32:13 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(ActorRolloutRefWorker pid=3656852)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x1463275fa7a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656321)[0m Total steps: -1, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656321)[0m INFO 04-23 03:32:07 config.py:1670] Downcasting torch.float32 to torch.bfloat16.[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656851)[0m local rank 0
[36m(ActorRolloutRefWorker pid=3656321)[0m before init cache memory allocated: 4.071012352GB, reserved: 4.223664128GB
[36m(ActorRolloutRefWorker pid=3656321)[0m after init cache memory allocated: 35.251468288GB, reserved: 35.404120064GB
[36m(ActorRolloutRefWorker pid=3656321)[0m INFO 04-23 03:32:14 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656321)[0m WARNING 04-23 03:32:14 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656321)[0m local rank 0[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656852)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}
[36m(ActorRolloutRefWorker pid=3656321)[0m After building vllm rollout, memory allocated (GB): 29.51576519012451, memory reserved (GB): 32.97265625
[36m(ActorRolloutRefWorker pid=3656321)[0m After building sharding manager, memory allocated (GB): 29.51576519012451, memory reserved (GB): 32.97265625
[1/1] Start to process.
[1/1] Start to generate.
3168
[2025-04-23 03:34:04,523][datasets][INFO] - PyTorch version 2.4.0+cu124 available.
+------------+--------------------------+
| Metric     | Value                    |
+============+==========================+
| model_path | l3lab/L1-Qwen-1.5B-Exact |
+------------+--------------------------+
| dataset    | gpqa.parquet             |
+------------+--------------------------+
| pass@1     | 0.2389520202020202       |
+------------+--------------------------+
| pass@16    | 0.7222222222222222       |
+------------+--------------------------+
[36m(ActorRolloutRefWorker pid=3656850)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656850)[0m   warnings.warn([32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3656850)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
++ date +%s
+ end_time=1745379254
+ duration=182
++ date
+ echo 'Finished generation for gpqa at Wed Apr 23 03:34:14 AM UTC 2025'
Finished generation for gpqa at Wed Apr 23 03:34:14 AM UTC 2025
+ echo 'Total generation time for gpqa: 182 seconds'
Total generation time for gpqa: 182 seconds
+ echo ----------------------------------------
----------------------------------------
+ for DATA_TYPE in "${DATATYPES[@]}"
++ date
+ echo 'Starting generation for lsat at Wed Apr 23 03:34:14 AM UTC 2025'
Starting generation for lsat at Wed Apr 23 03:34:14 AM UTC 2025
++ date +%s
+ start_time=1745379254
+ mkdir -p /lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024
+ python3 -m verl.trainer.main_generation trainer.nnodes=1 trainer.n_gpus_per_node=8 data.path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/data/data_1024/lsat.parquet data.output_path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024/lsat.parquet data.n_samples=16 data.batch_size=2048 model.path=l3lab/L1-Qwen-1.5B-Exact rollout.temperature=0.6 rollout.response_length=2048 rollout.top_k=-1 rollout.top_p=0.95 rollout.gpu_memory_utilization=0.9 rollout.tensor_model_parallel_size=1
/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
2025-04-23 03:34:32,441	INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.140.49.233:6379...
2025-04-23 03:34:32,456	INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[36m(pid=3671412)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3671412)[0m No module named 'vllm._version'
[36m(pid=3671412)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3671819)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3671819)[0m No module named 'vllm._version'
[36m(pid=3671819)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3671822)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3671822)[0m No module named 'vllm._version'
[36m(pid=3671822)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(ActorRolloutRefWorker pid=3671819)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3671819)[0m ERROR:2025-04-23 03:34:48,957:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3671819)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3671819)[0m ERROR:2025-04-23 03:34:49,044:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3671819)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(ActorRolloutRefWorker pid=3671819)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ActorRolloutRefWorker pid=3671819)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.32it/s]
[36m(ActorRolloutRefWorker pid=3671819)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.11it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.97it/s]
[36m(pid=3671821)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=3671821)[0m No module named 'vllm._version'[32m [repeated 5x across cluster][0m
[36m(pid=3671821)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 5x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671819)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 15x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671819)[0m ERROR:2025-04-23 03:35:08,182:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 15x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.71s/it][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.25s/it][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671824)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(ActorRolloutRefWorker pid=3671824)[0m   warnings.warn(
[36m(ActorRolloutRefWorker pid=3671412)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m ERROR:2025-04-23 03:35:08,647:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 7x across cluster][0m
{'actor': {'fsdp_config': {'fsdp_size': -1,
                           'grad_offload': False,
                           'optimizer_offload': False,
                           'param_offload': False,
                           'wrap_policy': {'min_num_params': 0}},
           'optim': {'lr': 1e-06,
                     'lr_warmup_steps_ratio': 0.0,
                     'min_lr_ratio': None,
                     'total_training_steps': -1,
                     'warmup_style': 'constant'},
           'strategy': 'fsdp',
           'ulysses_sequence_parallel_size': 1},
 'data': {'batch_size': 2048,
          'data_source_key': 'data_source',
          'n_samples': 16,
          'output_path': '/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024/lsat.parquet',
          'path': '/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/data/data_1024/lsat.parquet',
          'prompt_key': 'prompt',
          'response_key': 'responses',
          'reward_model_key': 'reward_model'},
 'model': {'external_lib': None, 'path': 'l3lab/L1-Qwen-1.5B-Exact'},
 'rollout': {'do_sample': True,
             'dtype': 'bfloat16',
             'enable_chunked_prefill': True,
             'enforce_eager': True,
             'free_cache_engine': True,
             'gpu_memory_utilization': 0.9,
             'ignore_eos': False,
             'load_format': 'dummy_dtensor',
             'log_prob_micro_batch_size': 8,
             'max_num_batched_tokens': 8192,
             'max_num_seqs': 1024,
             'micro_batch_size': 256,
             'n': 1,
             'n_val': 1,
             'name': 'vllm',
             'prompt_length': 1536,
             'response_length': 2048,
             'temperature': 0.6,
             'tensor_model_parallel_size': 1,
             'top_k': -1,
             'top_p': 0.95},
 'trainer': {'n_gpus_per_node': 8, 'nnodes': 1}}
[36m(ActorRolloutRefWorker pid=3671412)[0m Model config after override: Qwen2Config {
[36m(ActorRolloutRefWorker pid=3671412)[0m   "_name_or_path": "l3lab/L1-Qwen-1.5B-Exact",
[36m(ActorRolloutRefWorker pid=3671412)[0m   "architectures": [
[36m(ActorRolloutRefWorker pid=3671412)[0m     "Qwen2ForCausalLM"
[36m(ActorRolloutRefWorker pid=3671412)[0m   ],
[36m(ActorRolloutRefWorker pid=3671412)[0m   "attention_dropout": 0.0,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "bos_token_id": 151646,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "eos_token_id": 151643,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "hidden_act": "silu",
[36m(ActorRolloutRefWorker pid=3671412)[0m   "hidden_size": 1536,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "initializer_range": 0.02,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "intermediate_size": 8960,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "max_position_embeddings": 131072,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "max_window_layers": 21,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "model_type": "qwen2",
[36m(ActorRolloutRefWorker pid=3671412)[0m   "num_attention_heads": 12,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "num_hidden_layers": 28,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "num_key_value_heads": 2,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "pad_token_id": 151643,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "rms_norm_eps": 1e-06,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "rope_scaling": null,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "rope_theta": 10000,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "sliding_window": null,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "tie_word_embeddings": false,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "torch_dtype": "float32",
[36m(ActorRolloutRefWorker pid=3671412)[0m   "transformers_version": "4.47.1",
[36m(ActorRolloutRefWorker pid=3671412)[0m   "use_cache": true,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "use_mrope": false,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "use_sliding_window": false,
[36m(ActorRolloutRefWorker pid=3671412)[0m   "vocab_size": 151936
[36m(ActorRolloutRefWorker pid=3671412)[0m }
[36m(ActorRolloutRefWorker pid=3671412)[0m 
[36m(ActorRolloutRefWorker pid=3671412)[0m Qwen2ForCausalLM contains 1.78B parameters
[36m(ActorRolloutRefWorker pid=3671412)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x14d508ff67a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(ActorRolloutRefWorker pid=3671819)[0m Total steps: -1, num_warmup_steps: 0
[36m(ActorRolloutRefWorker pid=3671819)[0m INFO 04-23 03:35:08 config.py:1670] Downcasting torch.float32 to torch.bfloat16.
[36m(ActorRolloutRefWorker pid=3671412)[0m Before building vllm rollout, memory allocated (GB): 0.4375143051147461, memory reserved (GB): 3.2421875
[36m(ActorRolloutRefWorker pid=3671819)[0m INFO 04-23 03:35:14 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(ActorRolloutRefWorker pid=3671819)[0m WARNING 04-23 03:35:14 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(ActorRolloutRefWorker pid=3671821)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x1497e50e67a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m Total steps: -1, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m INFO 04-23 03:35:08 config.py:1670] Downcasting torch.float32 to torch.bfloat16.[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671819)[0m local rank 0
[36m(ActorRolloutRefWorker pid=3671412)[0m before init cache memory allocated: 4.071012352GB, reserved: 4.223664128GB
[36m(ActorRolloutRefWorker pid=3671412)[0m after init cache memory allocated: 35.251468288GB, reserved: 35.404120064GB
[36m(ActorRolloutRefWorker pid=3671412)[0m INFO 04-23 03:35:16 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m WARNING 04-23 03:35:16 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m local rank 0[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671824)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}
[1/1] Start to process.
[36m(ActorRolloutRefWorker pid=3671412)[0m After building vllm rollout, memory allocated (GB): 29.51576519012451, memory reserved (GB): 32.97265625
[36m(ActorRolloutRefWorker pid=3671412)[0m After building sharding manager, memory allocated (GB): 29.51576519012451, memory reserved (GB): 32.97265625
[1/1] Start to generate.
3680
Error executing job with overrides: ['trainer.nnodes=1', 'trainer.n_gpus_per_node=8', 'data.path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/data/data_1024/lsat.parquet', 'data.output_path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024/lsat.parquet', 'data.n_samples=16', 'data.batch_size=2048', 'model.path=l3lab/L1-Qwen-1.5B-Exact', 'rollout.temperature=0.6', 'rollout.response_length=2048', 'rollout.top_k=-1', 'rollout.top_p=0.95', 'rollout.gpu_memory_utilization=0.9', 'rollout.tensor_model_parallel_size=1']
[36m(ActorRolloutRefWorker pid=3671825)[0m Traceback (most recent call last):
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/worker/model_runner_base.py", line 116, in _wrapper
[36m(ActorRolloutRefWorker pid=3671825)[0m     return func(*args, **kwargs)
[36m(ActorRolloutRefWorker pid=3671825)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1708, in execute_model
[36m(ActorRolloutRefWorker pid=3671825)[0m     output: SamplerOutput = self.model.sample(
[36m(ActorRolloutRefWorker pid=3671825)[0m                             ^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 433, in sample
[36m(ActorRolloutRefWorker pid=3671825)[0m     next_tokens = self.sampler(logits, sampling_metadata)
[36m(ActorRolloutRefWorker pid=3671825)[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(ActorRolloutRefWorker pid=3671825)[0m     return self._call_impl(*args, **kwargs)
[36m(ActorRolloutRefWorker pid=3671825)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(ActorRolloutRefWorker pid=3671825)[0m     return forward_call(*args, **kwargs)
[36m(ActorRolloutRefWorker pid=3671825)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py", line 231, in forward
[36m(ActorRolloutRefWorker pid=3671825)[0m     self._init_sampling_tensors(logits, sampling_metadata)
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py", line 195, in _init_sampling_tensors
[36m(ActorRolloutRefWorker pid=3671825)[0m     do_min_p) = SamplingTensors.from_sampling_metadata(
[36m(ActorRolloutRefWorker pid=3671825)[0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/sampling_metadata.py", line 471, in from_sampling_metadata
[36m(ActorRolloutRefWorker pid=3671825)[0m     sampling_tensors = SamplingTensors.from_lists(
[36m(ActorRolloutRefWorker pid=3671825)[0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/sampling_metadata.py", line 529, in from_lists
[36m(ActorRolloutRefWorker pid=3671825)[0m     temperatures_t = torch.tensor(
[36m(ActorRolloutRefWorker pid=3671825)[0m                      ^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m RuntimeError: CUDA error: an illegal memory access was encountered
[36m(ActorRolloutRefWorker pid=3671825)[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[36m(ActorRolloutRefWorker pid=3671825)[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[36m(ActorRolloutRefWorker pid=3671825)[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[36m(ActorRolloutRefWorker pid=3671825)[0m 
[36m(ActorRolloutRefWorker pid=3671825)[0m 
[36m(ActorRolloutRefWorker pid=3671825)[0m The above exception was the direct cause of the following exception:
[36m(ActorRolloutRefWorker pid=3671825)[0m 
[36m(ActorRolloutRefWorker pid=3671825)[0m Traceback (most recent call last):
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 205, in generate_sequences
[36m(ActorRolloutRefWorker pid=3671825)[0m     output = self.inference_engine.generate(
[36m(ActorRolloutRefWorker pid=3671825)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/utils.py", line 1063, in inner
[36m(ActorRolloutRefWorker pid=3671825)[0m     return fn(*args, **kwargs)
[36m(ActorRolloutRefWorker pid=3671825)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 353, in generate
[36m(ActorRolloutRefWorker pid=3671825)[0m     outputs = self._run_engine(use_tqdm=use_tqdm)
[36m(ActorRolloutRefWorker pid=3671825)[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 166, in _run_engine
[36m(ActorRolloutRefWorker pid=3671825)[0m     outputs = super()._run_engine(use_tqdm=use_tqdm)
[36m(ActorRolloutRefWorker pid=3671825)[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 879, in _run_engine
[36m(ActorRolloutRefWorker pid=3671825)[0m     step_outputs = self.llm_engine.step()
[36m(ActorRolloutRefWorker pid=3671825)[0m                    ^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 1386, in step
[36m(ActorRolloutRefWorker pid=3671825)[0m     outputs = self.model_executor.execute_model(
[36m(ActorRolloutRefWorker pid=3671825)[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 163, in execute_model
[36m(ActorRolloutRefWorker pid=3671825)[0m     all_outputs = self.worker.execute_model(execute_model_req=execute_model_req)
[36m(ActorRolloutRefWorker pid=3671825)[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/third_party/vllm/vllm_v_0_6_3/worker.py", line 267, in execute_model
[36m(ActorRolloutRefWorker pid=3671825)[0m     return self.model_runner.execute_model(
[36m(ActorRolloutRefWorker pid=3671825)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[36m(ActorRolloutRefWorker pid=3671825)[0m     return func(*args, **kwargs)
[36m(ActorRolloutRefWorker pid=3671825)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(ActorRolloutRefWorker pid=3671825)[0m   File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/worker/model_runner_base.py", line 146, in _wrapper
[36m(ActorRolloutRefWorker pid=3671825)[0m     raise type(err)(f"Error in model execution: "
[36m(ActorRolloutRefWorker pid=3671825)[0m RuntimeError: Error in model execution: CUDA error: an illegal memory access was encountered
[36m(ActorRolloutRefWorker pid=3671825)[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[36m(ActorRolloutRefWorker pid=3671825)[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[36m(ActorRolloutRefWorker pid=3671825)[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[36m(ActorRolloutRefWorker pid=3671825)[0m 
[36m(ActorRolloutRefWorker pid=3671412)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3671412)[0m   warnings.warn([32m [repeated 7x across cluster][0m
Traceback (most recent call last):
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/trainer/main_generation.py", line 124, in main
    output = wg.generate_sequences(data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/cache/verl/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/cache/verl/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/cache/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/cache/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): [36mray::ActorRolloutRefWorker.generate_sequences()[39m (pid=3671825, ip=10.140.49.233, actor_id=d764f221b43852b5784121a40c000000, repr=<verl.workers.fsdp_workers.ActorRolloutRefWorker object at 0x14b19ea3f710>)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1708, in execute_model
    output: SamplerOutput = self.model.sample(
                            ^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 433, in sample
    next_tokens = self.sampler(logits, sampling_metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py", line 231, in forward
    self._init_sampling_tensors(logits, sampling_metadata)
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py", line 195, in _init_sampling_tensors
    do_min_p) = SamplingTensors.from_sampling_metadata(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/sampling_metadata.py", line 471, in from_sampling_metadata
    sampling_tensors = SamplingTensors.from_lists(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/model_executor/sampling_metadata.py", line 529, in from_lists
    temperatures_t = torch.tensor(
                     ^^^^^^^^^^^^^
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::ActorRolloutRefWorker.generate_sequences()[39m (pid=3671825, ip=10.140.49.233, actor_id=d764f221b43852b5784121a40c000000, repr=<verl.workers.fsdp_workers.ActorRolloutRefWorker object at 0x14b19ea3f710>)
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 205, in generate_sequences
    output = self.inference_engine.generate(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/utils.py", line 1063, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 353, in generate
    outputs = self._run_engine(use_tqdm=use_tqdm)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 166, in _run_engine
    outputs = super()._run_engine(use_tqdm=use_tqdm)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 879, in _run_engine
    step_outputs = self.llm_engine.step()
                   ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 1386, in step
    outputs = self.model_executor.execute_model(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 163, in execute_model
    all_outputs = self.worker.execute_model(execute_model_req=execute_model_req)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/third_party/vllm/vllm_v_0_6_3/worker.py", line 267, in execute_model
    return self.model_runner.execute_model(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/worker/model_runner_base.py", line 146, in _wrapper
    raise type(err)(f"Error in model execution: "
RuntimeError: Error in model execution: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

[36mray::ActorRolloutRefWorker.generate_sequences()[39m (pid=3671825, ip=10.140.49.233, actor_id=d764f221b43852b5784121a40c000000, repr=<verl.workers.fsdp_workers.ActorRolloutRefWorker object at 0x14b19ea3f710>)
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/workers/fsdp_workers.py", line 443, in generate_sequences
    output = self.rollout.generate_sequences(prompts=prompts)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 274, in generate_sequences
    torch.cuda.empty_cache()
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/cuda/memory.py", line 170, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

[36mray::ActorRolloutRefWorker.generate_sequences()[39m (pid=3671825, ip=10.140.49.233, actor_id=d764f221b43852b5784121a40c000000, repr=<verl.workers.fsdp_workers.ActorRolloutRefWorker object at 0x14b19ea3f710>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/workers/fsdp_workers.py", line 439, in generate_sequences
    with self.rollout_sharding_manager:
  File "/lus/eagle/projects/argonne_tpc/chen/repo/deepscaler/verl/verl/workers/sharding_manager/fsdp_vllm.py", line 105, in __exit__
    torch.cuda.empty_cache()
  File "/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/cuda/memory.py", line 170, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(ActorRolloutRefWorker pid=3671825)[0m INFO 04-23 03:35:31 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250423-033531.pkl...
[36m(ActorRolloutRefWorker pid=3671825)[0m WARNING 04-23 03:35:31 model_runner_base.py:143] Failed to pickle inputs of failed execution: CUDA error: an illegal memory access was encountered
[36m(ActorRolloutRefWorker pid=3671825)[0m WARNING 04-23 03:35:31 model_runner_base.py:143] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[36m(ActorRolloutRefWorker pid=3671825)[0m WARNING 04-23 03:35:31 model_runner_base.py:143] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[36m(ActorRolloutRefWorker pid=3671825)[0m WARNING 04-23 03:35:31 model_runner_base.py:143] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[36m(ActorRolloutRefWorker pid=3671825)[0m WARNING 04-23 03:35:31 model_runner_base.py:143] 
[36m(ActorRolloutRefWorker pid=3671825)[0m Restarting vLLM due to error:  Error in model execution: CUDA error: an illegal memory access was encountered
[36m(ActorRolloutRefWorker pid=3671825)[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[36m(ActorRolloutRefWorker pid=3671825)[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[36m(ActorRolloutRefWorker pid=3671825)[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[36m(ActorRolloutRefWorker pid=3671825)[0m 
[36m(ActorRolloutRefWorker pid=3671825)[0m Retrying...
[36m(ActorRolloutRefWorker pid=3671412)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
++ date +%s
+ end_time=1745379334
+ duration=80
++ date
+ echo 'Finished generation for lsat at Wed Apr 23 03:35:34 AM UTC 2025'
Finished generation for lsat at Wed Apr 23 03:35:34 AM UTC 2025
+ echo 'Total generation time for lsat: 80 seconds'
Total generation time for lsat: 80 seconds
+ echo ----------------------------------------
----------------------------------------
+ for DATA_TYPE in "${DATATYPES[@]}"
++ date
+ echo 'Starting generation for mmlu_1000 at Wed Apr 23 03:35:34 AM UTC 2025'
Starting generation for mmlu_1000 at Wed Apr 23 03:35:34 AM UTC 2025
++ date +%s
+ start_time=1745379334
+ mkdir -p /lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024
+ python3 -m verl.trainer.main_generation trainer.nnodes=1 trainer.n_gpus_per_node=8 data.path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/data/data_1024/mmlu_1000.parquet data.output_path=/lus/eagle/projects/argonne_tpc/chen/repo/myfork/l1/cache/output/output_1024/mmlu_1000.parquet data.n_samples=16 data.batch_size=2048 model.path=l3lab/L1-Qwen-1.5B-Exact rollout.temperature=0.6 rollout.response_length=2048 rollout.top_k=-1 rollout.top_p=0.95 rollout.gpu_memory_utilization=0.9 rollout.tensor_model_parallel_size=1
/home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
ANTLR runtime and generated code versions disagree: 4.7.2!=4.9.3
2025-04-23 03:35:52,598	INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.140.49.233:6379...
2025-04-23 03:35:52,611	INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[36m(pid=3679335)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3679335)[0m No module named 'vllm._version'
[36m(pid=3679335)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3681771)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3681771)[0m No module named 'vllm._version'
[36m(pid=3681771)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3681772)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3681772)[0m No module named 'vllm._version'
[36m(pid=3681772)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(ActorRolloutRefWorker pid=3681771)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3681771)[0m ERROR:2025-04-23 03:36:09,683:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3681771)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3681771)[0m ERROR:2025-04-23 03:36:09,762:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/adapter_config.json'
[36m(ActorRolloutRefWorker pid=3681771)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(ActorRolloutRefWorker pid=3681771)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(ActorRolloutRefWorker pid=3681771)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.12it/s]
[36m(ActorRolloutRefWorker pid=3681771)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.61it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.52it/s]
[36m(pid=3681770)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=3681770)[0m No module named 'vllm._version'[32m [repeated 5x across cluster][0m
[36m(pid=3681770)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 5x across cluster][0m
[36m(ActorRolloutRefWorker pid=3681776)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 15x across cluster][0m
[36m(ActorRolloutRefWorker pid=3681776)[0m ERROR:2025-04-23 03:36:29,150:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 15x across cluster][0m
[36m(ActorRolloutRefWorker pid=3679335)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3679335)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3679335)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.88s/it][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3679335)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.35s/it][32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3681776)[0m /home/lechen/.local/sophia/conda/2024-08-08/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(ActorRolloutRefWorker pid=3681776)[0m   warnings.warn(
[36m(ActorRolloutRefWorker pid=3681772)[0m Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 7x across cluster][0m
[36m(ActorRolloutRefWorker pid=3681772)[0m ERROR:2025-04-23 03:36:30,575:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/eagle/argonne_tpc/model_weights/hub/models--l3lab--L1-Qwen-1.5B-Exact/.no_exist/b1fa57f192f0b14bd033d0085faaa80cdc39694b/preprocessor_config.json'[32m [repeated 7x across cluster][0m
